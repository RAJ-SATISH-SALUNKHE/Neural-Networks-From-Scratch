{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting to Neural Network Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {s:i for i, s in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". : e\n",
      "e : m\n",
      "m : m\n",
      "m : a\n",
      "a : .\n"
     ]
    }
   ],
   "source": [
    "# Create a training set for bigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(f\"{ch1} : {ch2}\")\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xs and ys are the indices for the characters\n",
    "\n",
    "And, we simply cannot give indices as input to the neural network\n",
    "\n",
    "So we will perform one hot encoding to feed the character data to the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 27])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "         -2.9643e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
       "         -4.7125e-01,  7.8682e-01, -3.2843e-01, -4.3297e-01,  1.3729e+00,\n",
       "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4039e-01,\n",
       "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0730e-02,\n",
       "          2.4968e+00,  2.4448e+00],\n",
       "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
       "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "         -2.1259e+00,  9.6041e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
       "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
       "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
       "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
       "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
       "          5.4108e-01, -1.1646e+00]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Torch.randn() returns a tensor filled with random numbers from a normal distribution with mean  = 0 and variance  = 1\"\"\"\n",
    "\n",
    "\"\"\"Let's say we have a word emma. there are 5 bigrams for the word emma\n",
    "   so the dimensions of xenc would be (5, 27)\n",
    "\n",
    "   So, when we do xenc @ W , we are doing => (5, 27) @ (27, 1), which results to (5, 1)\n",
    "\"\"\"\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination of above step\n",
    "\n",
    "lets say we access the following way out of xenc @ W\n",
    "\n",
    "(xenc @ W)[3, 13] = > this is giving us the firing rate of the 13th neuron looking at the third input\n",
    "\n",
    "For everyone of the 27 neurons we created, what is the firing rate of those neurons on everyone of the 5 examples (5 wrt emma bigram)\n",
    "\n",
    "This is the dumbest, smallest, simplest neural network with a single linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we are trying to create a probabilty distribution for the next input in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are now treating these 27 outputs as log_counts and so we exponentiate every element'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We are now treating these 27 outputs as log_counts and so we exponentiate every element\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax !! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = (xenc @ W) #log-counts\n",
    "counts = logits.exp() # Equivalent to the N matrix in the bigram language model using BOW\n",
    " \n",
    "probs = counts / counts.sum(1, keepdims=True) # Normalising the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0123, 0.0181, 0.0267, 0.0737, 0.0150], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This step tells what probabilty is the input giving for the expected output character\n",
    "\"\"\"\n",
    "\n",
    "probs[torch.arange(len(xs)), ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss\n",
    "\n",
    "loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Torch.randn() returns a tensor filled with random numbers from a normal distribution with mean  = 0 and variance  = 1\"\"\"\n",
    "\n",
    "\"\"\"Let's say we have a word emma. there are 5 bigrams for the word emma\n",
    "   so the dimensions of xenc would be (5, 27)\n",
    "\n",
    "   So, when we do xenc @ W , we are doing => (5, 27) @ (27, 1), which results to (5, 1)\n",
    "\"\"\"\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W #log-counts\n",
    "counts = logits.exp() # Equivalent to the N matrix in the bigram language model using BOW\n",
    "probs = counts / counts.sum(1, keepdims=True) # Normalising the counts\n",
    "\n",
    "loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.grad = None\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7693, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 228146\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "\n",
    "print(f\"Number of examples: {num}\")\n",
    "\n",
    "\n",
    "# Initialize the network\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.768618583679199\n",
      "3.3788068294525146\n",
      "3.161090850830078\n",
      "3.0271859169006348\n",
      "2.9344840049743652\n",
      "2.867231607437134\n",
      "2.8166542053222656\n",
      "2.777146577835083\n",
      "2.745253801345825\n",
      "2.7188305854797363\n",
      "2.696505308151245\n",
      "2.6773719787597656\n",
      "2.6608052253723145\n",
      "2.6463515758514404\n",
      "2.633665084838867\n",
      "2.622471570968628\n",
      "2.6125476360321045\n",
      "2.6037068367004395\n",
      "2.595794916152954\n",
      "2.5886807441711426\n",
      "2.5822560787200928\n",
      "2.576429843902588\n",
      "2.5711236000061035\n",
      "2.566272735595703\n",
      "2.5618226528167725\n",
      "2.5577261447906494\n",
      "2.5539445877075195\n",
      "2.550442695617676\n",
      "2.547192335128784\n",
      "2.5441696643829346\n",
      "2.5413522720336914\n",
      "2.538722038269043\n",
      "2.536262035369873\n",
      "2.5339579582214355\n",
      "2.531797409057617\n",
      "2.529768228530884\n",
      "2.527860164642334\n",
      "2.5260636806488037\n",
      "2.5243704319000244\n",
      "2.522773265838623\n",
      "2.52126407623291\n",
      "2.519836664199829\n",
      "2.5184857845306396\n",
      "2.5172054767608643\n",
      "2.515990734100342\n",
      "2.5148372650146484\n",
      "2.5137410163879395\n",
      "2.512697696685791\n",
      "2.511704921722412\n",
      "2.5107579231262207\n",
      "2.509855031967163\n",
      "2.5089924335479736\n",
      "2.5081686973571777\n",
      "2.507380247116089\n",
      "2.5066256523132324\n",
      "2.5059030055999756\n",
      "2.5052106380462646\n",
      "2.5045459270477295\n",
      "2.5039076805114746\n",
      "2.503295421600342\n",
      "2.5027060508728027\n",
      "2.5021398067474365\n",
      "2.501594305038452\n",
      "2.5010695457458496\n",
      "2.500562906265259\n",
      "2.500075578689575\n",
      "2.4996049404144287\n",
      "2.499150514602661\n",
      "2.4987120628356934\n",
      "2.49828839302063\n",
      "2.4978787899017334\n",
      "2.497483015060425\n",
      "2.4970996379852295\n",
      "2.4967293739318848\n",
      "2.496370315551758\n",
      "2.4960227012634277\n",
      "2.4956860542297363\n",
      "2.4953596591949463\n",
      "2.4950432777404785\n",
      "2.4947361946105957\n",
      "2.494438886642456\n",
      "2.494149684906006\n",
      "2.4938690662384033\n",
      "2.4935965538024902\n",
      "2.4933321475982666\n",
      "2.493075132369995\n",
      "2.4928252696990967\n",
      "2.492582321166992\n",
      "2.4923462867736816\n",
      "2.492116928100586\n",
      "2.4918932914733887\n",
      "2.491675853729248\n",
      "2.491464376449585\n",
      "2.491258382797241\n",
      "2.491058111190796\n",
      "2.4908626079559326\n",
      "2.4906723499298096\n",
      "2.4904870986938477\n",
      "2.4903063774108887\n",
      "2.490130662918091\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "regularization_strength = 0.01\n",
    "\n",
    "for k in range(100):\n",
    "     \n",
    "     # forward pass\n",
    "    xenc   = F.one_hot(xs, num_classes=27).float()                              # input to the network : one-hot encoding\n",
    "    logits = xenc @ W                                                           # predict log-counts\n",
    "    counts = logits.exp()                                                       # counts, equivalent to N\n",
    "    probs  = counts/ counts.sum(1, keepdims=True)                               # Probabilities for next character\n",
    "\n",
    "    loss   = -probs[torch.arange(num), ys].log().mean() + regularization_strength * (W**2).mean()  # Loss  + Regularization loss\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward Pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update\n",
    "    W.data += -50 * W.grad\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did model smoothening by adding fake counts in the BOW model\n",
    "\n",
    "But in the NN framework, we can do this by pushing W close to 0\n",
    "\n",
    "If W is 0, logits = 0, \n",
    "counts is all 1, \n",
    "and prob distribution is very smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junide.\n",
      "janasah.\n",
      "p.\n",
      "cfay.\n",
      "a.\n",
      "nn.\n",
      "kohin.\n",
      "tolian.\n",
      "juwe.\n",
      "kalanaauranilevias.\n",
      "dedainrwieta.\n",
      "ssonielylarte.\n",
      "faveumerifontume.\n",
      "phynslenaruani.\n",
      "core.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(15):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        # Forward pass\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts/ counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
